## Group No 106

## Group Member Names:
1. Anandi SIngh
2. Kashish Rajput
3. Nitu Sharma
4. Saurav Sharma
# 1. Import the required libraries
##---------Type the code below this line------------------##
import os
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras import layers
import matplotlib.pyplot as plt
import matplotlib.image as implt
import seaborn as sns
# 2. Data Acquisition  -- Score: 0.5 Mark
 
For the problem identified by you, students have to find the data source themselves from any data source.

## 2.1 Code for converting the above downloaded data into a form suitable for DL


train_path = './horses_or_humans/training/'
test_path = './horses_or_humans/testing/'
category_names = ['horses', 'humans']
##---------Type the code below this line------------------##
dataset_name = 'horses_or_humans'
data_dir = tf.keras.utils.get_file(
    fname='C:/Users/SRIVATS/Downloads/horses_or_humans'+dataset_name+'.zip',
    origin='https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip',
    extract=False
)
test_imgs = {}
test = [len(os.listdir(test_path + '/' + category)) for category in category_names]
test_imgs = {category: count for category, count in zip(category_names, test)}

sns.barplot(x=test, y=category_names).set_title('Number of testing data per class')

train_imgs = {}
train = [len(os.listdir(train_path + '/' + category)) for category in category_names]
train_imgs = {category: count for category, count in zip(category_names, train)}

sns.barplot(x=train, y=category_names).set_title('Number of training data per class')

total_train_data = train_imgs
total_test_data = test_imgs
print(f"Total Trainig data: {total_train_data} \nTotal Testing Data: {total_test_data}")

train_horses = os.path.join(train_path, 'horses')
train_humans = os.path.join(train_path, 'humans')

img1 = plt.imread(os.path.join(train_horses, "horse02-0.png"))
img2 = plt.imread(os.path.join(train_humans, "human02-00.png"))

plt.subplot(1, 2, 1)
plt.title('horse')
plt.imshow(img1)
plt.subplot(1, 2, 2)
plt.title('human')
plt.imshow(img2)
plt.show()

## 2.1 Write your observations from the above. 

1. Size of the dataset
2. What type of data attributes are there?
3. What are you classifying?
4. Plot the distribution of the categories of the target / label. 


# 1. Size of the dataset
In training dataset size is as follows :-

number of horses = 500

number of humans = 527

Total size of training data = 1027 images of horses and humans. 

In testing dataset size is as follows :-

number of horses = 128

number of humans = 128

Total size of testing data = 256 images of horses and humans.

2. Types of data attributes 
For the "horses vs humans" dataset, the attributes associated with each image are not defined or with the dataset.The dataset primarily consists of images of horses and humans without any attributes. However we can extract some attributes using deep learning model. Some of the attributes can be Pixel values, shape features, text features etc. Images contain rendered images of horses and humans in RGB format

3. What are you classifying ?
For the "horses vs humans" dataset, the typical task is to perform binary classification, distinguishing between images of horses and images of humans. The goal is to build a model that can accurately classify new unseen images as either a horse or a human.

4. Plot the distribution of target varaible
From above plot we notice that there are more number of humans as compared to horses in training dataset.


# 3. Data Preparation -- Score: 1 Mark

Perform the data prepracessing that is required for the data that you have downloaded. 


This stage depends on the dataset that is used. 
## 3.1 Apply pre-processing techiniques

* to remove duplicate data
* to impute or remove missing data
* to remove data inconsistencies
* Encode categorical data
* Normalize the data
* Feature Engineering
* Stop word removal, lemmatiation, stemming, vectorization


IF ANY
##---------Type the code below this line------------------##
train_datagen = ImageDataGenerator(
    rescale=1/255.0,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(300, 300),
    batch_size=128,
    class_mode='binary'
)

validation_datagen = ImageDataGenerator(rescale=1/255.0)

validation_generator = validation_datagen.flow_from_directory(
    test_path,
    target_size=(300, 300),
    batch_size=128,
    class_mode='binary'
)

train_generator.class_indices
1. Image Resizing: The images in the dataset are resized to a desired input shape (150x150). Resizing helps to standardize the image dimensions, ensuring consistency in the input size for the model.
2. Image Normalization: The pixel values of the images are normalized to a range of 0 to 1. This is achieved by dividing each pixel value by 255.0. Normalizing the pixel values helps to bring them to a common scale and assists in faster convergence during training.

## 3.3 Split the data into training set and testing set
## 3.4 Preprocessing report

Mention the method adopted  and justify why the method was used
* to remove duplicate data, if present 
* to impute or remove missing data, if present 
* to remove data inconsistencies, if present 
* to encode categorical data 
* the normalization technique used

If the any of the above are not present, then also add in the report below.

Report the size of the training dataset and testing dataset

1. Duplicate Data: Not applicable

2. Missing Data: Not applicable 

3. Data Inconsistencies: Not applicable 

4. Categorical Data Encoding: Not applicable

5. Image Resizing: The images in the dataset are resized to a desired input shape (150x150). Resizing helps to standardize the image dimensions, ensuring consistency in the input size for the model.

6. Image Normalization: The pixel values of the images are normalized to a range of 0 to 1. This is achieved by dividing each pixel value by 255.0. Normalizing the pixel values helps to bring them to a common scale and assists in faster convergence during training.

By performing image resizing, image normalization, the code prepares the train and test data for training an image classification model on the "horses vs humans" dataset.
# 4. Deep Neural Network Architecture - Score:  Marks

## 4.1 Design the architecture that you will be using

* Sequential Model Building with Activation for each layer.
* Add dense layers, specifying the number of units in each layer and the activation function used in the layer.
* Use Relu Activation function in each hidden layer
* Use Sigmoid / softmax Activation function in the output layer as required

DO NOT USE CNN OR RNN. 
##---------Type the code below this line------------------##
model = keras.Sequential()
model.add(keras.layers.Input(shape=(300, 300, 3)))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(400, activation='relu'))
model.add(keras.layers.Dense(300, activation='relu'))
model.add(keras.layers.Dense(250, activation='relu'))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.build(input_shape=(None, 300, 300, 3))
## 4.2 DNN Report

Report the following and provide justification for the same.



* Number of layers
* Number of units in each layer
* Total number of trainable parameters 


##---------Type the answer below this line------------------##
model.summary()
# 5. Training the model - Score: 1 Mark

## 5.1 Configure the training

Configure  the model for training, by using appropriate optimizers and regularizations

Compile with categorical CE loss and metric accuracy.
from tensorflow.keras.optimizers import SGD

# Create SGD optimizer
sgd = SGD(learning_rate=0.01)

# Compile the model with categorical cross-entropy loss and metric accuracy
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

## 5.2 Train the model

Train Model with cross validation, with total time taken shown for 20 epochs.

Use SGD.
import time
##---------Type the code below this line------------------##
import time

optimizer = keras.optimizers.SGD(learning_rate=0.01)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

start_time = time.time()
history = model.fit(
    train_generator,
    epochs=20,
    verbose=1,
    validation_data=validation_generator
)
end_time = time.time()

total_time = end_time - start_time
print(f"Total time for training: {total_time}")

## Justify your choice of optimizers and regulizations used and the hyperparameters tuned
Choice and Justification:

Optimizer:

Choice: SGD (Stochastic Gradient Descent) optimizer.
Justification: SGD is chosen for its simplicity, efficiency, and widespread use in neural network training. It updates model parameters based on gradients computed from mini-batches of the training data. The learning rate is set to 0.01 for efficient parameter updates.
Regularization:

Choice: Implicit regularization through dropout layers.
Justification: While no explicit regularization techniques like L1 or L2 regularization are used, dropout layers are included in the model architecture. Dropout acts as implicit regularization by randomly dropping neurons during training, reducing co-adaptation and preventing overfitting. This helps the model generalize well to unseen data.
Hyperparameters Tuned:

Hyperparameters: Learning rate and number of epochs.
Justification: The learning rate is set to 0.01, a common initial value for SGD optimization. It determines the step size for parameter updates. The number of epochs is set to 20 to strike a balance between underfitting and overfitting. This allows the model to learn from the data without excessive training.
Overall, the chosen optimizer, implicit regularization through dropout, and reasonable hyperparameter settings provide a good starting point for training the model. These choices balance simplicity, efficiency, and generalization capabilities. Fine-tuning and experimentation with the hyperparameters may be required to achieve optimal performance for the specific dataset and problem at hand.

# 6. Test the model - 0.5 marks

##---------Type the code below this line------------------##
loss, accuracy = model.evaluate(validation_generator)
print(f"Loss = {loss}, accuracy = {accuracy}")
# 7. Intermediate result  - Score: 1 mark

1. Plot the training and validation accuracy history.
2. Plot the training and validation loss history. 
3. Report the testing accuracy and loss.
4. Show Confusion Matrix for testing dataset.
5. Report values for preformance study metrics like accuracy, precision, recall, F1 Score.

def plot_metric(history, metric):
    metric_values = history.history[metric]
    val_metric_values = history.history['val_' + metric]

    epochs = range(1, len(metric_values) + 1)

    plt.plot(epochs, metric_values, 'b', label='Train ' + metric)
    plt.plot(epochs, val_metric_values, 'r', label='Validation ' + metric)
    plt.title('Model ' + metric)
    plt.xlabel('Epochs')
    plt.ylabel(metric)
    plt.legend()
    plt.show()

loss, accuracy = model.evaluate(validation_generator)
print(f"Loss = {loss}, Accuracy = {accuracy}")

plot_metric(history, "loss")
plot_metric(history, "accuracy")
# 8. Model architecture - Score: 1 mark


Modify the architecture designed in section 4.1 

1. by decreasing one layer
2. by increasing one layer

For example, if the architecture in 4.1 has 5 layers, then 8.1 should have 4 layers and 8.2 should have 6 layers.

Plot the comparison of the training and validation accuracy of the three architecures (4.1, 8.1 and 8.2)



# reduced layer
model_2 = keras.Sequential([
    keras.Input(shape=(300,300,3)),
    layers.Flatten(),
    layers.Dense(400, activation='relu'),
    layers.Dense(300, activation='relu'),
    layers.Dense(250, activation='relu'),
    layers.Dense(1, activation='sigmoid'),
])

model_2.build((300,300,3))
model_2.summary()
optimiser = keras.optimizers.SGD()
model_2.compile(loss='binary_crossentropy', optimizer=optimiser, metrics=['accuracy'])
start_time = time.time()
history_2 = model_2.fit(
    train_generator,
    epochs = 20, 
    verbose = 1,
    validation_data = validation_generator
)
end_time = time.time()
print(f"Total time for training: {end_time-start_time}")
# increased 1 layer
model_3 = keras.Sequential([
    keras.Input(shape=(300,300,3)),
    layers.Flatten(),
    layers.Dense(400, activation='relu'),
    layers.Dense(300, activation='relu'),
    layers.Dense(250, activation='relu'),
    layers.Dense(200, activation='relu'),
    layers.Dense(100, activation='relu'),
    layers.Dense(1, activation='sigmoid'),
])

model_3.build((300,300,3))
model_3.summary()
optimiser = keras.optimizers.SGD()
model_3.compile(loss='binary_crossentropy', optimizer=optimiser, metrics=['accuracy'])
start_time = time.time()
history_3 = model_3.fit(
    train_generator,
    epochs = 20, 
    verbose = 1,
    validation_data = validation_generator
)
end_time = time.time()
print(f"Total time for training: {end_time-start_time}")
loss, accuracy = model.evaluate(validation_generator)
loss2, accuracy2 = model_2.evaluate(validation_generator)
loss3, accuracy3 = model_3.evaluate(validation_generator)
print(f"Loss_original = {loss}, accuracy_original = {accuracy}")
print(f"Loss with 1 less layer = {loss2}, accuracy with 1 less layer = {accuracy2}")
print(f"Loss with 1 extra layer = {loss3}, accuracy with 1 extra layer = {accuracy3}")
plt.figure(figsize=(16,9))
plt.plot(history.history['accuracy'], label='Original model training accuracy', color='b', linestyle='-')
plt.plot(history.history['val_accuracy'], label='Original model validation accuracy', color='orange', linestyle='-')
plt.plot(history_2.history['accuracy'], label='model2 training accuracy', color='b', linestyle='--')
plt.plot(history_2.history['val_accuracy'], label='model2 validation accuracy', color='orange', linestyle='--')
plt.plot(history_3.history['accuracy'], label='model3 training accuracy', color='b', linestyle='-.')
plt.plot(history_3.history['val_accuracy'], label='model3 validation accuracy', color='orange', linestyle='-.')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid()
# 9. Regularisations - Score: 1 mark

Modify the architecture designed in section 4.1

1. Dropout of ratio 0.25
2. Dropout of ratio 0.25 with L2 regulariser with factor 1eâˆ’04. 

Plot the comparison of the training and validation accuracy of the three (4.1, 9.1 and 9.2)


##---------Type the code below this line------------------##
model_4 = keras.Sequential([
    keras.Input(shape=(300,300,3)),
    layers.Flatten(),
    layers.Dense(400, activation='relu'),
    layers.Dense(300, activation='relu'),
    layers.Dropout(0.25),
    layers.Dense(250, activation='relu'),
    layers.Dense(100, activation='relu'),
    layers.Dense(1, activation='sigmoid'),
])

model_4.build((300,300,3))
model_4.summary()
optimiser = keras.optimizers.SGD()
model_4.compile(loss='binary_crossentropy', optimizer=optimiser, metrics=['accuracy'])
start_time = time.time()
history_4 = model_4.fit(
    train_generator,
    epochs = 20, 
    verbose = 1,
    validation_data = validation_generator
)
end_time = time.time()
print(f"Total time for training: {end_time-start_time}")

model_5 = keras.Sequential([
    keras.Input(shape=(300,300,3)),
    layers.Flatten(),
    layers.Dense(400, activation='relu'),
    layers.Dense(300, activation='relu', kernel_regularizer=keras.regularizers.L2(0.0001), activity_regularizer=keras.regularizers.L2(0.0001)),
    layers.Dropout(0.25),
    layers.Dense(250, activation='relu'),
    layers.Dense(100, activation='relu'),
    layers.Dense(1, activation='sigmoid'),
])

model_5.build((300,300,3))
model_5.summary()
optimiser = keras.optimizers.SGD()
model_5.compile(loss='binary_crossentropy', optimizer=optimiser, metrics=['accuracy'])
start_time = time.time()
history_5 = model_5.fit(
    train_generator,
    epochs = 20, 
    verbose = 1,
    validation_data = validation_generator
)
end_time = time.time()
print(f"Total time for training: {end_time-start_time}")
loss, accuracy = model.evaluate(validation_generator)
loss4, accuracy4 = model_4.evaluate(validation_generator)
loss5, accuracy5 = model_5.evaluate(validation_generator)
print(f"Loss_original = {loss}, accuracy_original = {accuracy}")
print(f"Loss with Dropout layer = {loss4}, accuracy with Dropout layer = {accuracy4}")
print(f"Loss with Dropout layer and regularisation = {loss5}, accuracy with Dropout layer and regularisation = {accuracy5}")
plt.figure(figsize=(16,9))
plt.plot(history.history['accuracy'], label='Original model training accuracy', color='b', linestyle='-')
plt.plot(history.history['val_accuracy'], label='Original model validation accuracy', color='orange', linestyle='-')
plt.plot(history_4.history['accuracy'], label='model4 training accuracy', color='b', linestyle='--')
plt.plot(history_4.history['val_accuracy'], label='model4 validation accuracy', color='orange', linestyle='--')
plt.plot(history_5.history['accuracy'], label='model5 training accuracy', color='b', linestyle='-.')
plt.plot(history_5.history['val_accuracy'], label='model5 validation accuracy', color='orange', linestyle='-.')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid()
# 10. Optimisers -Score: 1 mark

Modify the code written in section 5.2

1. RMSProp with your choice of hyper parameters
2. Adam with your choice of hyper parameters

Plot the comparison of the training and validation accuracy of the three (5.2, 10.1 and 10.2)

1. RMSProp with your choice of hyper parameters - Model-6
import time
from tensorflow.keras.optimizers import RMSprop

optimizer = RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07)

model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

start_time = time.time()
history = model.fit(train_generator, epochs=20, verbose=1, validation_data=validation_generator)
end_time = time.time()

print(f"Total time for training: {end_time-start_time}")
loss, accuracy = model.evaluate(validation_generator)
print(f"Loss = {loss}, accuracy = {accuracy}")

2. Adam with your choice of hyper parameters - Model - 7
##---------Type the code below this line------------------##
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
start_time = time.time()
history = model.fit(
    train_generator,
    epochs = 20, 
    verbose = 1,
    validation_data = validation_generator
)
end_time = time.time()
print(f"Total time for training: {end_time-start_time}")
loss, accuracy = model.evaluate(validation_generator)
print(f"Loss = {loss}, accuracy = {accuracy}")


# 11. Conclusion - Score: 1 mark

Comparing the sections 4.1, 5.2, 8, 9, and 10, present your observations on which model or architecture or regualiser or optimiser perfomed better.

# Observations
Observations:

Model 1: SGD optimizer

Train accuracy: 0.5764
Train loss: 0.6786
Validation accuracy: 0.5
Validation loss: 0.7902
Test accuracy: 0.5
Test loss: 0.7902

Model 2: Decreased one layer from Model 1
Train accuracy: 0.5764
Train loss: 0.6786
Validation accuracy: 0.5
Validation loss: 0.7902
Test accuracy: 0.5
Test loss: 0.7534

Model 3: Increased one layer from Model 1
Train accuracy: 0.5560
Train loss: 0.6711
Validation accuracy: 0.5
Validation loss: 0.7534
Test accuracy: 0.5862
Test loss: 0.7081

Model 4: Dropout layer (ratio 0.25)
Train accuracy: 0.5560
Train loss: 0.6787
Validation accuracy: 0.6016
Validation loss: 0.6523
Test accuracy: 0.6016
Test loss: 0.6523

Model 5: Dropout layer (ratio 0.25) with L2 regularization (factor 1e-04)
Train accuracy: 0.6115
Train loss: 0.7061
Validation accuracy: 0.8555
Validation loss: 0.6579
Test accuracy: 0.8555
Test loss: 0.6579

Model 6: RMSProp optimizer with custom hyperparameters
Train accuracy: 0.5131
Train loss: 0.6928
Validation accuracy: 0.5
Validation loss: 0.6936
Test accuracy: 0.5
Test loss: 0.6936

Model 7: Adam optimizer with custom hyperparameters
Train accuracy: 0.5131
Train loss: 0.6931
Validation accuracy: 0.5
Validation loss: 0.6932
Test accuracy: 0.5
Test loss: 0.6932
## Comparison and conlusion of models
Comparison and Observations:

Model 5, which includes a dropout layer (ratio 0.25) and L2 regularization (factor 1e-04), achieved the highest validation and test accuracy. This indicates that the regularization techniques helped improve generalization performance, reducing overfitting.

Model 4, which includes only a dropout layer (ratio 0.25), also performed reasonably well with a similar test accuracy, but without the regularization term.

Models 1, 2, 6, and 7, which used different optimizers or made architectural changes, did not show significant improvement compared to the baseline (Model 1) in terms of accuracy. Their test accuracy remained at 0.5, indicating poor performance.

Model 3, which increased one layer from the baseline, showed a slightly higher test accuracy than the baseline but still performed relatively poorly compared to Models
